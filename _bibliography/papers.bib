---
---
@article{yang2025layeranimate,
  author    = {Yang, Yuxue and Fan, Lue and Lin, Zuzeng and Wang, Feng and Zhang†, Zhaoxiang},
  title     = {LayerAnimate: Layer-specific Control for Animation},
  journal   = {arXiv preprint arXiv:2501.08295},
  year      = {2025},
  abstract  = {Animation separates foreground and background elements into layers, with distinct processes for sketching, refining, coloring, and in-betweening. Existing video generation methods typically treat animation as a monolithic data domain, lacking fine-grained control over individual layers. In this paper, we introduce LayerAnimate, a novel architectural approach that enhances fine-grained control over individual animation layers within a video diffusion model, allowing users to independently manipulate foreground and background elements in distinct layers. To address the challenge of limited layer-specific data, we propose a data curation pipeline that features automated element segmentation, motion-state hierarchical merging, and motion coherence refinement. Through quantitative and qualitative comparisons, and user study, we demonstrate that LayerAnimate outperforms current methods in terms of animation quality, control precision, and usability, making it an ideal tool for both professional animators and amateur enthusiasts. This framework opens up new possibilities for layer-specific animation applications and creative flexibility. Our code is available at https://layeranimate.github.io.},
  selected  = {true},
  code      = "https://github.com/IamCreateAI/LayerAnimate",
  preview   = {layeranimate.png},
  html      = "https://layeranimate.github.io/",
  arxiv     = {2501.08295},
  annotation={† Corresponding authors}
}

@article{fan2024trim,
  author    = {Fan*, Lue and Yang*, Yuxue and Li, Minxing and Li†, Hongsheng and Zhang†, Zhaoxiang},
  title     = {Trim 3D Gaussian Splatting for Accurate Geometry Representation},
  journal   = {arXiv preprint arXiv:2406.07499},
  year      = {2024},
  abstract  = {In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to reconstruct accurate 3D geometry from images. Previous arts for geometry reconstruction from 3D Gaussians mainly focus on exploring strong geometry regularization. Instead, from a fresh perspective, we propose to obtain accurate 3D geometry of a scene by Gaussian trimming, which selectively removes the inaccurate geometry while preserving accurate structures. To achieve this, we analyze the contributions of individual 3D Gaussians and propose a contribution-based trimming strategy to remove the redundant or inaccurate Gaussians. Furthermore, our experimental and theoretical analyses reveal that a relatively small Gaussian scale is a non-negligible factor in representing and optimizing the intricate details. Therefore the proposed TrimGS maintains relatively small Gaussian scales. In addition, TrimGS is also compatible with the effective geometry regularization strategies in previous arts. When combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS consistently yields more accurate geometry and higher perceptual quality.},
  selected  = {true},
  code      = "https://github.com/YuxueYang1204/TrimGS",
  preview   = {trimgs_teaser.png},
  html      = "https://trimgs.github.io/",
  arxiv     = {2406.07499},
  annotation={* Equal contribution<br>† Corresponding authors}
}

@inproceedings{
	yang2024mixsup,
	title={MixSup: Mixed-grained Supervision for Label-efficient Li{DAR}-based 3D Object Detection},
	author={Yang, Yuxue and Fan†, Lue and Zhang†, Zhaoxiang},
	booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
	year={2024},
  abstract  = {Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality. We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors. MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations. Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden. The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup.},
	abbr={ICLR},
  selected={true},
  code="https://github.com/BraveGroup/PointSAM-for-MixSup",
  preview={MixSup_teaser.png},
  html="https://openreview.net/forum?id=Q1vkAhdI6j",
  arxiv={2401.16305},
  annotation={† Corresponding authors}
}


@InProceedings{Fan_2023_ICCV,
    author    = {Fan, Lue and Yang, Yuxue and Mao, Yiming and Wang, Feng and Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},
    title     = {Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {19820-19829},
    arxiv     = {2304.12315},
    abstract  = {This paper aims for high-performance offline LiDAR-based 3D object detection. We first observe that experienced human annotators annotate objects from a track-centric perspective. They first label objects in a track with clear shapes, and then leverage the temporal coherence to infer the annotations of obscure objects. Drawing inspiration from this, we propose a high-performance offline detector in a track-centric perspective instead of the conventional object-centric perspective. Our method features a bidirectional tracking module and a track-centric learning module. Such a design allows our detector to infer and refine a complete track once the object is detected at a certain moment. We refer to this characteristic as "onCe detecTed, neveR Lost" and name the proposed system CTRL. Extensive experiments demonstrate the remarkable performance of our method, surpassing the human-level annotating accuracy and outperforming the previous state-of-the-art methods in the highly competitive Waymo Open Dataset leaderboard without model ensemble. The code is available at https://github.com/tusen-ai/SST.},
    code="https://github.com/tusen-ai/SST",
    selected={true},
    html="https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.html",
    abbr={ICCV},
    preview={CTRL_teaser.png}
}

@ARTICLE{Fan_2023_TPAMI,
  author={Fan, Lue and Yang, Yuxue and Wang, Feng and Wang, Naiyan and Zhang, Zhaoxiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Super Sparse 3D Object Detection},
  year={2023},
  volume={45},
  number={10},
  pages={12490-12505},
  keywords={Feature extraction;Detectors;Point cloud compression;Three-dimensional displays;Proposals;Object detection;Laser radar;3D object detection;autonomous driving;instance segmentation;LiDAR;point clustering;sparse;temporal fusion;waymo open dataset},
  doi={10.1109/TPAMI.2023.3286409},
  abstract={As the perception range of LiDAR expands, LiDAR-based 3D object detection contributes ever-increasingly to the long-range perception in autonomous driving. Mainstream 3D object detectors often build dense feature maps, where the cost is quadratic to the perception range, making them hardly scale up to the long-range settings. To enable efficient long-range detection, we first propose a fully sparse object detector termed FSD. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR groups the points into instances and applies highly-efficient instance-wise feature extraction. The instance-wise grouping sidesteps the issue of the center feature missing, which hinders the design of the fully sparse architecture. To further enjoy the benefit of fully sparse characteristic, we leverage temporal information to remove data redundancy and propose a super sparse detector named FSD++. FSD++ first generates residual points, which indicate the point changes between consecutive frames. The residual points, along with a few previous foreground points, form the super sparse input data, greatly reducing data redundancy and computational overhead. We comprehensively analyze our method on the large-scale Waymo Open Dataset, and state-of-the-art performance is reported. To showcase the superiority of our method in long-range detection, we also conduct experiments on Argoverse 2 Dataset, where the perception range ( 200 m) is much larger than Waymo Open Dataset ( 75 m).},
  code="https://github.com/tusen-ai/SST",
  selected={true},
  arxiv={2301.02562},
  html="https://ieeexplore.ieee.org/document/10153690",
  abbr={TPAMI},
  preview="FSD++_teaser.png"
}




@InProceedings{Hu_2023_CVPR,
    author    = {Hu, Zhengxi and Yang, Yuxue and Zhai, Xiaolin and Yang, Dingye and Zhou, Bohan and Liu, Jingtai},
    title     = {GFIE: A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {8907-8916},
    abstract  = {Gaze-following is a kind of research that requires locating where the person in the scene is looking automatically under the topic of gaze estimation. It is an important clue for understanding human intention, such as identifying objects or regions of interest to humans. However, a survey of datasets used for gaze-following tasks reveals defects in the way they collect gaze point labels. Manual labeling may introduce subjective bias and is labor-intensive, while automatic labeling with an eye-tracking device would alter the person's appearance. In this work, we introduce GFIE, a novel dataset recorded by a gaze data collection system we developed. The system is constructed with two devices, an Azure Kinect and a laser rangefinder, which generate the laser spot to steer the subject's attention as they perform in front of the camera. And an algorithm is developed to locate laser spots in images for annotating 2D/3D gaze targets and removing ground truth introduced by the spots. The whole procedure of collecting gaze behavior allows us to obtain unbiased labels in unconstrained environments semi-automatically. We also propose a baseline method with stereo field-of-view (FoV) perception for establishing a 2D/3D gaze-following benchmark on the GFIE dataset. Project page: https://sites.google.com/view/gfie.},
    abbr      = {CVPR},
    code      = "https://github.com/nkuhzx/GFIE",
    selected  = {true},
    html   = "https://openaccess.thecvf.com/content/CVPR2023/html/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.html",
    preview={GFIE_teaser.png}

}


